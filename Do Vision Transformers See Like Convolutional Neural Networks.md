## 摘要
卷积神经网络（CNNs）至今已成为可视化数据的实际模型。 最近的研究表明，视觉变换模型(VIT)在图像分类任务中可以达到相当甚至更好的性能。 这就提出了一个核心问题：Vision Transformer是如何解决这些任务的？ 它们的行为像卷积网络，还是学习完全不同的视觉表征？ 通过分析VITS和CNNS在图像分类基准上的内部表示结构，我们发现这两种体系结构之间存在显著的差异，如VIT在所有层次上都具有更统一的表征。 我们探索了这些差异是如何产生的，发现了自我注意所起的关键作用，这使得全局信息能够早期聚集，以及VIT残差连接，这将特征从较低层传播到较高层。 我们研究了空间定位的分支，证明VITS成功地保留了输入的空间信息，不同分类方法的效果显著。 最后，我们研究了（预训练）数据集规模对中间特征和迁移学习的影响，并讨论了与MLP-Mixer等新体系结构的连接。

## 引言
在过去的几年里，深度学习在视觉任务上的成功主要依赖于卷积神经网络[20,16]。 这在很大程度上是由于卷积层编码的空间等效性的强大归纳偏置，其是学习通用视觉表征的关键，为了易转移，强性能。 然而，值得注意的是，最近的工作已经证明，Transformer神经网络能够在大规模图像分类任务中具有同等或更好的性能[14]。 这些Vision Transformer(VIT)的操作几乎与语言中使用的Transformer相同[13]，使用自注意力，而不是卷积，在不同的位置聚集信息。 这与先前的大量工作形成鲜明对比，这些工作侧重于更明确地纳入图像特定的归纳偏置[30,9,4]

这一突破突出了一个基本问题：Vision Transformer是如何解决这些基于图像的任务的？ 他们的行为像卷曲一样，从零开始学习同样的归纳偏置吗？ 或者他们正在发展新的任务表征？ 尺度在学习这些表征中的作用是什么？ 下游任务有分支吗？ 本文对这些问题进行了研究，揭示了VITS和CNNS之间的主要表征差异，这些差异产生的原因，以及它们对分类和迁移学习的影响。 具体而言，我们的贡献是： 

- 我们研究了VITs和CNNs的内部表征结构，发现两个模型之间有显著的差异，如VIT具有更统一的表征，较低层和较高层之间有更大的相似性。 
- 分析如何利用局部/全局空间信息，我们发现VIT在低层比RESNET包含更多的全局信息，导致定量不同的特征。 
- 然而，我们发现在较低的层次上整合局部信息仍然是至关重要的，大规模的预训练数据有助于早期注意层学习这样做 
- 我们研究了VIT统一的内部结构，发现跳跃连接在VIT中的影响甚至比在ResNets中更大，对性能和表征相似性有很强的影响。 
- 基于目标检测的潜在应用，我们研究了输入空间信息的保存情况，发现了空间定位和分类方法之间的联系。 
- 我们研究了数据集规模对迁移学习的影响，通过线性探针研究揭示了它对高质量中间表征的重要性。 

## 相关工作
开发非卷积神经网络来处理计算机视觉任务，特别是Transformer神经网络[44]一直是一个活跃的研究领域。 先前的研究着眼于局部多头自注意力，借鉴卷积感受野的结构[30，36]，直接将CNNS与自注意力结合[4，2，46]或将Transformer应用于较小尺寸的图像[6，9]。 与这些相比，Vision Transformer[14]对Transformer体系结构进行的修改甚至更少，使得与CNNS进行比较特别有趣。 自从VIT的发展以来，也有最近的工作分析VIT的各个方面，特别是鲁棒性[3,31,28]和自我监督的影响[5,7]。 最近的其他相关工作着眼于设计混合VIT-CNN模型[49，11]，利用模型之间的结构差异。 Transformer和CNNs之间的比较最近也在文本领域进行了研究[41]。 

我们的工作集中在VITS的表征结构上。 为了研究VIT表示，我们借鉴了神经网络表示相似性的技术，它允许对神经网络内和网络间的表征进行定量比较[17,34,26,19]。 这些技术非常成功地提供了关于不同视觉体系结构的特性[29，22，18]、语言模型中的表征结构[48，25，47，21]、训练方法的动态[33，24]和特定领域模型行为[27，35，38]的见解。 在我们的研究中，我们还应用了线性探针，它已经被证明对分析视觉[1]和文本[8，32，45]模型中的学习表征是有用的。 

## 背景和实验设置
我们的目标是了解与CNNS相比，VITS在表示和解决图像任务的方式上是否存在差异。 基于Dosovitskiy等人的结果。 [14]，我们选取了一组具有代表性的CNN和VIT模型--RESNET50x1、RESNET152x2、VIT-B/32、VIT-B/16、VIT-L/16和VIT-H/14。 除非另有说明，模型是在JFT-300M数据集[40]上训练的，尽管我们也研究了在ImageNet ILSVRC 2012数据集[12,37]和标准迁移学习基准[50,14]上训练的模型。 我们使用各种分析方法来研究这些模型的层表示，对这些模型如何发挥作用有了许多见解。 我们在附录A中提供了实验设置的进一步细节。 

**表征相似性和CKA（Centered Kernel Alignment）：** 分析神经网络的（隐藏）层表示是一个挑战，因为它们的特征分布在大量神经元上。 这种分布式方面也使得很难有意义地比较跨神经网络的表示。 Centered Kernel Alignment(CKA)[17,10]解决了这些问题，使网络内部和网络之间的表示能够进行定量比较。 具体而言，CKA以两层神经元的表示（激活矩阵）$x∈\mathbb{R}^{m\times p_{1}}$ 和$y∈\mathbb{R}^{m\times p_{2}}$作为输入，分别以$p_{1}$和$p_{2}$神经元在相同的M个例子上进行计算。 让$K=XX^{T}$和$L=YY^{T}$表示两个层的Gram矩阵（它根据层表征来度量一对数据点的相似性），CKA计算： 

$$
\operatorname{CKA}(\boldsymbol{K}, \boldsymbol{L})=\frac{\operatorname{HSIC}(\boldsymbol{K}, \boldsymbol{L})}{\sqrt{\operatorname{HSIC}(\boldsymbol{K}, \boldsymbol{K}) \operatorname{HSIC}(\boldsymbol{L}, \boldsymbol{L})}}
$$

其中HSIC是Hilbert-Schmidt独立性准则[15]。 给定中心矩阵$\boldsymbol{H}=\boldsymbol{I}\_{n}-\frac{1}{n} \mathbf{1}\mathbf{1}^{\top}$和中心Gram矩阵$K'=HKH$和$L'=HLH,HSIC(K,L)=vec(K')\cdot vec(L')/(m-1)^2$,给出了这些定心革兰矩阵之间的相似性。 CKA对表示的正交变换（包括神经元排列）是不变的，归一化项保证了对各向同性标度的不变性。 这些性质使得神经网络隐藏表示能够进行有意义的比较和分析。 为了对我们的模型和任务进行大规模工作，我们使用Minibables近似HSIC[39]的无偏估计，如[29]所建议的。 
![1_2](https://user-images.githubusercontent.com/108515137/176866170-68291214-fe13-42ec-92ff-86b650b43e32.png)

## **😋😋😋Please donate a little star for me😋😋😋**

