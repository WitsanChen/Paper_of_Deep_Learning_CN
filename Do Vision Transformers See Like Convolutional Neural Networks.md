**摘要**：卷积神经网络（CNNs）至今已成为可视化数据的实际模型。 最近的研究表明，视觉变换模型(VIT)在图像分类任务中可以达到相当甚至更好的性能。 这就提出了一个核心问题：Vision Transformer是如何解决这些任务的？ 它们的行为像卷积网络，还是学习完全不同的视觉表征？ 通过分析VITS和CNNS在图像分类基准上的内部表示结构，我们发现这两种体系结构之间存在显著的差异，如VIT在所有层次上都具有更统一的表征。 我们探索了这些差异是如何产生的，发现了自我注意所起的关键作用，这使得全局信息能够早期聚集，以及VIT残差连接，这将特征从较低层传播到较高层。 我们研究了空间定位的分支，证明VITS成功地保留了输入的空间信息，不同分类方法的效果显著。 最后，我们研究了（预训练）数据集规模对中间特征和迁移学习的影响，并讨论了与MLP-Mixer等新体系结构的连接。

## 引言
在过去的几年里，深度学习在视觉任务上的成功主要依赖于卷积神经网络[20,16]。 这在很大程度上是由于卷积层编码的空间等效性的强大归纳偏置，其是学习通用视觉表征的关键，为了易转移，强性能。 然而，值得注意的是，最近的工作已经证明，Transformer神经网络能够在大规模图像分类任务中具有同等或更好的性能[14]。 这些Vision Transformer(VIT)的操作几乎与语言中使用的Transformer相同[13]，使用自注意力，而不是卷积，在不同的位置聚集信息。 这与先前的大量工作形成鲜明对比，这些工作侧重于更明确地纳入图像特定的归纳偏置[30,9,4]

这一突破突出了一个基本问题：Vision Transformer是如何解决这些基于图像的任务的？ 他们的行为像卷曲一样，从零开始学习同样的归纳偏置吗？ 或者他们正在发展新的任务表征？ 尺度在学习这些表征中的作用是什么？ 下游任务有分支吗？ 本文对这些问题进行了研究，揭示了VITS和CNNS之间的主要表征差异，这些差异产生的原因，以及它们对分类和迁移学习的影响。 具体而言，我们的贡献是： 
